\chapter{Experimental Results and Discussion} \label{Result}

\ifpdf
    \graphicspath{{chapter_5/figures/PNG/}{chapter_5/figures/PDF/}{chapter_5/figures/}}
\else
    \graphicspath{{chapter_5/figures/EPS/}{chapter_5/figures/}}
\fi

\section{Introduction}
In this section, we present the experimental results and analysis of our method. All the experiments were run five times and only the averages are reported as the classifiers used here are stochastic in nature. The classifiers were implemented using Scikit-learn library and Python 3.

\section{Result of used Algorithm}
The main goal of this work was to show that feature subspaced ensembles works better in comparison to the single classifiers. Without changing the feature subspacing as shown in the last section, we have use five different weak classifiers and their combinations in the ensemble. They are: Decision Tree (DT), Support Vector Machines (SVM), K-Nearest Neighbor (KNN), Logistic Regression (LR) and Naive Bayesian Classifier (NB). Note that we are performing 10-fold cross validation. Firstly, in Table~\ref{tabSingle}, we present the results achieved by each of these single classifiers on the dataset. Here, we note that performance of all the classifiers are similar. However, KNN performs little worse to other single classifiers. Note that the full set of features were fed to these single classifiers.




\begin{table}[h]
\centering
\begin{tabular}{l|p{1.5cm}p{1.2cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}} \hline
\bf Classifier & \bf Precision & \bf Recall & \bf F1 & \bf Acc & \bf MCC & \bf Sn& \bf Spc & %\bf TP & \bf FP & \bf FN  & \bf TN & 
\bf AuROC \\\hline
SVM & 0.8386 & 0.7152 & 0.7567 & 77.08 &  0.5686 & 80.61 & 74.41 & %207 & 71 & 43 & 179 & 
0.8528   \\\hline
NB & 0.6821 & 0.9440 & 0.7978 & 75.88 & 0.5614 & 68.84 & 91.32 & %143 & 13 & 107 & 237 & 
0.7588   \\\hline
KNN & 0.7431 & 0.3632 & 0.5007 & 64.50 & 0.3434 & 82.12 & 59.10 & %230 & 159 & 20 & 91 & 
0.7650   \\\hline
LR & 0.8605 & 0.7768 & 0.7869 & 78.92 & 0.5840 & 79.18 & 78.68 & %199 & 54 & 51 & 196 & 
0.8657  \\\hline
DT & 0.6760 & 0.7288 & 0.7302 & 73.36 & 0.4710 & 73.61 & 73.15 & %185 & 68 & 65 & 182 & 
0.7336  \\\hline
\end{tabular}
\caption{Performance of single classifiers on the dataset. \label{tabSingle}}
\end{table}



\begin{table}[h]
\centering
\begin{tabular}{l|p{1.5cm}p{1.2cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}} \hline
\bf Classifier & \bf Precision & \bf Recall & \bf F1 & \bf Acc& \bf MCC & \bf Sn& \bf Spc& %\bf True Positive & \bf False Positive & \bf False Negative  & \bf True Negative & 
\bf AuROC \\\hline
SVM+SVM+SVM & 0.9245 & 0.9512 & 0.8247 & 79.00& 0.6296 & 70.89& 97.24& %149 & 4 & 101 & 247 & 
0.9345   \\\hline
NB+NB+NB & 0.8234 & 0.9376 & 0.8234 & 79.80& 0.6230 & 73.29& 91.39 & %165 & 16 & 85 & 234 & 
0.8530   \\\hline
KNN+KNN+KNN & 0.9001 & 0.384 & 0.8151 & 78.64& 0.6036 & 71.94& 91.22& %159 & 15 & 91 & 235 & 
0.9166 \\\hline
LR+LR+LR & 0.8739 & 0.9220 & 0.8242 & 80.24 & 0.6083 & 74.35 & 89.96& %170 & 19 & 80 & 231 &
0.8969  \\\hline
DT+DT+DT & 0.8199 & 0.8288 & 0.8202 & 81.88& 0.6376 & 81.01 & 82.46 & %201 & 43 & 49 & 207 & 
0.8596  \\\hline
SVM+NB+LR & 0.8201 & 0.9736	& 0.7994 & 75.48& 0.5687 & 67.73 & 94.10 & %134 & 7 & 116 & 243 & 
0.8497 \\\hline
NB+LR+SVM & 0.8164 & 0.9736 & 0.7931 & 74.28& 0.5527 & 66.80 & 95.14 & %129 & 7 & 121 & 243 & 
0.8464 \\\hline
LR+SVM+NB & 0.8415 & 0.9776 & 0.8032 & 75.52 & 0.6539 & 67.81 & 95.99& %134 & 6 & 116 & 244 & 
0.8488 \\\hline
DT+SVM+DT & 0.7735 & 0.8944 & 0.8058 & 78.60 & 0.5912 & 73.49& 87.29 & %169 & 26 & 81 & 224 & 
0.8206 \\\hline
SVM+DT+DT & 0.7416 & 0.8928 & 0.8038 & 78.28 & 0.5854 & 73.18 & 87.04 & %168 & 27 & 82 & 223 & 
0.7616 \\\hline
LR+LR+DT &\ 0.8730 & 0.8816 &\bf  0.8360 & \bf 82.52&\bf  0.6603 & 79.51 & 86.52& %192 & 30 & 57 & 221 &
0.8819 \\\hline
SVM+LR+DT & 0.8186 & 0.9488 & 0.7969 & 77.48& 0.5882 & 70.39 & 92.13& %150 & 13 & 100 & 237 &
0.8379 \\\hline
SVM+NB+DT & 0.7530 & 0.9528 & 0.8098 & 77.44& 0.6087 & 70.23 & 92.66& %149 & 12 & 101 & 238 & 
0.7895 \\\hline
\end{tabular}
\caption{Performance of different ensemble classifiers on the dataset. \label{tabEnsemble}}
\end{table}

Next, we applied the ensemble classifier proposed in this project using each of these single classifiers member of the ensemble. The first five rows of Table~\ref{tabEnsemble} shows the experimental results. Note that, in each case accuracy and other measures were enhanced by the ensemble compared to the single classifier. The best performing ensemble was the decision tree ensemble.

We have also tried several mixtures of single classifiers in the ensemble. Eight different combinations of the five single classifiers were tried and the results are shown in the last eight rows of Table~\ref{tabEnsemble}. Here note that a combination of logistic regression and decision tree is the best performing ensemble combination among all in terms of F1, MCC and Accuracy. Thus we select this combination as the best ensemble for our method.

\subsection{Comparison with other methods}
We have also compared the performance of the best ensemble combination to that of the previous methods in the literature. We have used four other methods for the sake of comparison: AntiCP \cite{tyagi2013silico}, Hajisharifi's method \cite{hajisharifi2014predicting}, iACP \cite{chen2016iacp} and ACPred-FL \cite{wei2018acpred}. Note that, we have not run their predictors, rather reported the 10-fold cross validation results as presented in the work by Wei et al. \cite{wei2018acpred}. The results are shown in Table~\ref{tab:Compare}. 


\begin{table}[h]
    \centering
    \begin{tabular}{l| p{1.5cm} p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\hline
\bf Predictors	&\bf  Sn &\bf 	Spc &\bf  Acc &\bf MCC&\bf AuROC\\
\hline
AntiCP\_AAC&	67.2&	86.0&	76.6&	0.542&0.84\\%168	35	215	82
\hline
AntiCP\_DC&	73.6&	85.2&	79.4&	0.592	&0.87\\%184	37	213	66
\hline
Hajisharifiâ€™s method&	68.0&	86.4&	77.2&	0.553&0.84\\%	170	34	216	80
\hline
iACP&	72.8&	86.0&	79.4&	0.593&0.86\\%	182	35	215	68
\hline
ACPred-FL& 	84.8&	98.0&	91.4&	0.835	&0.94\\%212	5	245	38
\hline
Our Method & 79.5 & 86.5 & 82.5 & 0.660 & 0.88\\
\hline
 \end{tabular}
    \caption{Performance comparison of different state-of-the-art predictors on the benchmark dataset. \label{tab:Compare}}

\end{table}

Note that, the performance of our method is superior to all other methods except ACPred-FL. Now, ACPred-FL have used a large number of features compared to ours and also did perform feature selection that could have enhanced their performance.

\section{Summary}
In this chapter we know about results by using many algorithms and also know about the comparison of that used algorithms.